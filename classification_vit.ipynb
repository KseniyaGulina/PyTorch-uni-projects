{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMLTn60JXYCv"
      },
      "source": [
        "VIT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vk-sJ12mXpTE",
        "outputId": "6751c2bb-9f21-4e84-d2ee-5b579d127612"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m935.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jct9_v47XbKn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Compose, Resize, ToTensor\n",
        "from einops import rearrange, reduce, repeat\n",
        "from einops.layers.torch import Rearrange, Reduce\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.position_embeddings=nn.Parameter(torch.rand((img_size//patch_size)**2+1, embed_dim))\n",
        "        self.cls_token=nn.Parameter(torch.randn(1,1,embed_dim))\n",
        "\n",
        "\n",
        "        self.patch_embeddings = nn.Conv2d(in_chans,out_channels=embed_dim,\n",
        "                                          kernel_size=patch_size,\n",
        "                                          stride=patch_size)\n",
        "\n",
        "        self.projection=nn.Sequential(\n",
        "            nn.Conv2d(in_chans,embed_dim, kernel_size=patch_size,stride=patch_size),\n",
        "            Rearrange('b e h w -> b (h w) e'),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, image):\n",
        "\n",
        "\n",
        "        b,c,h,w=image.shape\n",
        "        x=self.projection(image)\n",
        "\n",
        "\n",
        "        cls_tokens=repeat(self.cls_token, '() n e -> b n e', b = b)\n",
        "        #patches = self.patch_embeddings(image).flatten(2).transpose(1,2)\n",
        "        x = torch.cat([cls_tokens,x],dim=1)\n",
        "\n",
        "\n",
        "        x+=self.position_embeddings\n",
        "\n",
        "        return x #patches\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, drop=0.):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        out_features = out_features or in_features\n",
        "        # Linear Layers\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1= nn.Linear(in_features, hidden_features)\n",
        "        self.fc2= nn.Linear(hidden_features, out_features)\n",
        "        # Activation(s)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        self.act=nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., out_drop=0.):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "\n",
        "\n",
        "        self.qkv = nn.Linear(dim,dim*3)\n",
        "\n",
        "        # reshape\n",
        "        # q&kT\n",
        "\n",
        "\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.out = nn.Linear(dim,dim)\n",
        "        self.out_drop = nn.Dropout(out_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        # Attention\n",
        "        x = self.qkv(x)\n",
        "\n",
        "        x = torch.reshape(x,( B, N, 3, self.num_heads, self.head_dim))\n",
        "\n",
        "        q = x[:,:,0]\n",
        "        k = x[:,:,1]\n",
        "        v = x[:,:,2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        # Out projection\n",
        "\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.out(x)\n",
        "        x = self.out_drop(x)\n",
        "\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, mlp_ratio=4, drop_rate=0.):\n",
        "        super().__init__()\n",
        "\n",
        "        # Normalization\n",
        "        self.norm1=nn.LayerNorm(dim)\n",
        "        self.norm2=nn.LayerNorm(dim)\n",
        "        # Attention\n",
        "        self.attention = Attention(dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.drop = nn.Dropout(drop_rate)\n",
        "        # Normalization\n",
        "\n",
        "\n",
        "        # MLP\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=3072, out_features=dim)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_plus = x\n",
        "        x = self.norm1(x)\n",
        "        # Attetnion\n",
        "        x = self.attention(x)\n",
        "\n",
        "        x = self.drop(x)\n",
        "        x = x + x_plus\n",
        "        x_plus = x\n",
        "        # MLP\n",
        "        x = self.norm2(x)\n",
        "        x = self.mlp(x)\n",
        "        x = self.drop(x)\n",
        "        x = x + x_plus\n",
        "        return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, depth, dim, num_heads=8, mlp_ratio=4, drop_rate=0.):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(dim, num_heads, mlp_ratio, drop_rate)\n",
        "            for i in range(depth)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "from torch.nn.modules.normalization import LayerNorm\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000,\n",
        "                 embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.,\n",
        "                 qkv_bias=False, drop_rate=0.,):\n",
        "        super().__init__()\n",
        "\n",
        "        # Присвоение переменных\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.in_chans = in_chans\n",
        "        self.num_classes = num_classes\n",
        "        self.embed_dim = embed_dim\n",
        "        self.depth = depth\n",
        "        self.num_heads = num_heads\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "\n",
        "        # Path Embeddings, CLS Token, Position Encoding\n",
        "        self.patch_embedding = PatchEmbedding(img_size=self.img_size, patch_size=self.patch_size, in_chans=self.in_chans, embed_dim=self.embed_dim)\n",
        "\n",
        "        # Transformer Encoder\n",
        "        self.transformer = Transformer(depth=self.depth, dim=self.embed_dim, num_heads=self.num_heads, mlp_ratio=self.mlp_ratio, drop_rate=0.)\n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Linear(in_features=self.embed_dim, out_features=self.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Path Embeddings, CLS Token, Position Encoding\n",
        "        x = self.patch_embedding(x)\n",
        "\n",
        "        # Transformer Encoder\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        # Classifier\n",
        "        x = self.classifier(x[:,0])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOLNXo4OSjql"
      },
      "source": [
        "init.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4MNAJySSNHp"
      },
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "classes = ('plane','car','bird','cat','deer','dog','frog','horse','ship','truck')\n",
        "\n",
        "dims = (3,32,32)\n",
        "num_classes = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WeC1yzFSr-m"
      },
      "source": [
        "data.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoVRMXyaTIkx"
      },
      "outputs": [],
      "source": [
        "!pip install lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yNkeelOS1jm"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\".\")\n",
        "\n",
        "#import classificator\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import lightning as L\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "PATH_DATASETS = os.environ.get('PATH_DATASETS',\".\")\n",
        "BATCH_SIZE = 256 if torch.cuda.is_available() else 64\n",
        "\n",
        "\n",
        "\n",
        "class L_data_module(L.LightningDataModule):\n",
        "    def __init__(self, data_dir: str = PATH_DATASETS):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))]\n",
        "        )\n",
        "        self.dims = dims\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def prepare_data(self):\n",
        "        CIFAR10(self.data_dir,train=True, download=True)\n",
        "        CIFAR10(self.data_dir,train=False, download=True)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        # Assign train/val datasets for use in dataloaders\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            cifar_full = CIFAR10(self.data_dir, train=True, transform=self.transform)\n",
        "\n",
        "            self.cifar_train, self.cifar_val = random_split(cifar_full, [45000,500])\n",
        "\n",
        "        # Assign test dataset for use in dataloader(s)\n",
        "        if stage == \"test\" or stage is None:\n",
        "            self.cifar_test = CIFAR10(self.data_dir, train=False, transform=self.transform)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.cifar_train, batch_size=BATCH_SIZE)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.cifar_val, batch_size=BATCH_SIZE)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.cifar_test, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbBhMSQAfL7M"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9YZQ7e-T218"
      },
      "source": [
        "model.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LopJOOdgT8eZ"
      },
      "outputs": [],
      "source": [
        "# import sys\n",
        "# sys.path.append(\".\")\n",
        "\n",
        "# import classificator\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import lightning as L\n",
        "from torchmetrics.functional import accuracy\n",
        "\n",
        "\n",
        "class L_model(L.LightningModule):\n",
        "    def __init__(self, channels, width, height, num_classes, hidden_size=64,lr=1e-4):\n",
        "        super().__init__()\n",
        "\n",
        "        # self.channels = channels\n",
        "        # self.width = width\n",
        "        # self.height = height\n",
        "        # self.hidden_size = hidden_size\n",
        "\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.lr = lr\n",
        "\n",
        "        # self.model = nn.Sequential(\n",
        "        #     nn.Flatten(),\n",
        "        #     nn.Linear(channels*width*height,hidden_size),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.Dropout(0.1),\n",
        "        #     nn.Linear(hidden_size,hidden_size),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.Dropout(0.1),\n",
        "        #     nn.Linear(hidden_size, num_classes),\n",
        "        # )\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            ViT(img_size=dims[-1], patch_size=16, in_chans=3, num_classes=self.num_classes,\n",
        "                 embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.,\n",
        "                 qkv_bias=False, drop_rate=0.,)\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        x = self.model(x)\n",
        "\n",
        "        return F.log_softmax(x,dim=1)\n",
        "    def training_step(self,batch):\n",
        "        x,y = batch\n",
        "\n",
        "        logits = self(x)\n",
        "        #print(logits.shape, y.shape)\n",
        "        loss = F.nll_loss(logits,y)\n",
        "\n",
        "        return loss\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x,y = batch\n",
        "\n",
        "        logits = self(x)\n",
        "        #print(logits.shape, y.shape)\n",
        "        loss = F.nll_loss(logits,y)\n",
        "        preds = torch.argmax(logits,dim=1)\n",
        "        acc = accuracy(preds,y, task='multiclass',num_classes=10)\n",
        "\n",
        "        self.log('val_loss',loss, prog_bar=True)\n",
        "        self.log('val_acc', acc,prog_bar=True)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "        return optimizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0WW6-nYUZ9g"
      },
      "source": [
        "train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEdNwlSUUr0u"
      },
      "outputs": [],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV7euW9WIJ62"
      },
      "source": [
        "4005e813f50f67317738fcec0baf6f1d026840f6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "dyhYNefhUUmm",
        "outputId": "1d4d0dfc-030a-4ae4-f329-5fc90dab02d0"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append(\".\")\n",
        "\n",
        "# import classificator.data as d\n",
        "# import classificator.model as model\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import lightning as L\n",
        "\n",
        "import wandb\n",
        "\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "run = wandb.init(project=\"cool-girly-pytorch-project\")\n",
        "\n",
        "wandb_logger = WandbLogger(name='ps4lr1-4',project=\"cool-girly-pytorch-project\", log_model='all')\n",
        "\n",
        "dm = L_data_module()\n",
        "model = L_model(3,32,32,num_classes=num_classes)\n",
        "#model = vit\n",
        "trainer = L.Trainer(\n",
        "    max_epochs=5,\n",
        "    accelerator='auto',\n",
        "    devices=1,\n",
        "    logger=wandb_logger,\n",
        ")\n",
        "trainer.fit(model,dm)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}